{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f807d73f-e1dc-4a78-a1a1-c6657b31d6f3",
   "metadata": {},
   "source": [
    "# Psych Scraper\n",
    "\n",
    "The goal is to download all of the Psych transcripts written by some random dude on some random [blog](https://jpgr.livejournal.com/) I found to be used as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4421f067-10d3-4842-b831-6d6286b160ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b883b38-2311-432f-93d1-41e7cead24b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for posts with the tag for the corresponding season\n",
    "search_urls = [f\"https://jpgr.livejournal.com/?tag=transcripts%3A%20psych%3A%20season%20{i}\" for i in range(1, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c4395332-0be7-4ee6-a7af-c746cd7b1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_post_urls(search_url):\n",
    "    result = []\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    post_link_els = soup.find_all(\"a\", class_=\"subj-link\")\n",
    "    result += [el[\"href\"] for el in post_link_els]\n",
    "\n",
    "    # need to check if second page of results exist\n",
    "    next_page_link_el = soup.find(\"a\", string=\"go earlier\")\n",
    "    if (next_page_link_el is not None):\n",
    "        next_page_url = next_page_link_el[\"href\"]\n",
    "        result += get_all_post_urls(next_page_url)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e199fada-4c5f-44b3-8334-4e75816265a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_urls = []\n",
    "for url in search_urls:\n",
    "    post_urls += get_all_post_urls(url)\n",
    "\n",
    "posts = [requests.get(url).text for url in post_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "fd5f5716-d4c8-4350-ac23-fe5e652a0941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in episode 3x09 Christmas Joy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3x09 Christmas Joy'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_episode_data(post):\n",
    "    soup = BeautifulSoup(post, \"html.parser\")\n",
    "    \n",
    "    subject = soup.find(\"div\", class_=\"subject\").string\n",
    "    i = re.search(r'\\dx\\d*', subject).start()\n",
    "    episode_title = subject[i:].strip()\n",
    "    \n",
    "    entry_text = soup.find(\"div\", class_=\"entry_text\")\n",
    "    tags = entry_text.find(\"div\", class_=\"ljtags\")\n",
    "    tags.extract()\n",
    "    for br in entry_text.find_all(\"br\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "\n",
    "    # The post content had formatting errors which the rendering engine dumps the error and raw content for\n",
    "    # what a pain    \n",
    "    for error in entry_text.find_all(\"div\", class_=\"ljparseerror\"):\n",
    "        print(f'Error in episode {episode_title}')\n",
    "        error.extract()\n",
    "    transcript_raw = entry_text.get_text()\n",
    "    transcript_raw = transcript_raw.replace(\"<b>\", \"\")\n",
    "    transcript_raw = transcript_raw.replace(\"</b>\", \"\")\n",
    "    transcript_raw = transcript_raw.replace(\"<i>\", \"\")\n",
    "    transcript_raw = transcript_raw.replace(\"</i>\", \"\")\n",
    "    transcript_raw = transcript_raw.replace(\"<u>\", \"\")\n",
    "    transcript_raw = transcript_raw.replace(\"</u>\", \"\")\n",
    "    \n",
    "    # Replace non-standard chars\n",
    "    transcript_raw = transcript_raw.replace(\"\\t\", \" \")\n",
    "    transcript_raw = transcript_raw.replace(\"–\", \"-\")\n",
    "    transcript_raw = transcript_raw.replace(\"—\", \"-\")\n",
    "    transcript_raw = transcript_raw.replace(\"‘\", \"'\")\n",
    "    transcript_raw = transcript_raw.replace(\"’\", \"'\")\n",
    "    transcript_raw = transcript_raw.replace(\"“\", \"\\\"\")\n",
    "    transcript_raw = transcript_raw.replace(\"”\", \"\\\"\")\n",
    "    transcript_raw = transcript_raw.replace(\"…\", \"...\")\n",
    "    transcript_raw = transcript_raw.replace(\"™\", \"...\")\n",
    "        \n",
    "    transcript_lines = [line.strip() for line in transcript_raw.split(\"\\n\")]\n",
    "    transcript = \"\\n\".join(transcript_lines).strip()\n",
    "        \n",
    "    return episode_title, transcript\n",
    "\n",
    "# get_episode_data(posts[-9])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "abe88949-12d7-49b1-af2f-a28cf0105e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in episode 2x9 Bounty Hunters!\n",
      "Error in episode 3x09 Christmas Joy\n",
      "Error in episode 3x05 Disco Didn't Die, It Was Murdered\n"
     ]
    }
   ],
   "source": [
    "episodes = [get_episode_data(post) for post in posts]\n",
    "episodes.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "59c613ea-0c2d-42b7-a1fe-8e13da0a0160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1x01 Pilot (1/2)\n",
      "1986, Santa Barbara, California\n",
      "\n",
      "INT. DINER, DAY\n",
      "\n",
      "A young boy of around 9 years old, SHAWN, sits across from his father, HENRY in a booth. HENRY is in his police\n",
      "\n",
      "uniform.\n",
      "\n",
      "HENRY:\n",
      "Did you do your home\n"
     ]
    }
   ],
   "source": [
    "print(episodes[0][0])\n",
    "print(episodes[0][1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c159b8a4-5130-4c08-933b-0a2cd781cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for title, transcript in episodes:\n",
    "    all_text += (f\"Episode {title}\\n\\n\")\n",
    "    all_text += transcript\n",
    "    all_text += \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "d6bea902-3271-4b0c-b6a0-280b20b8b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"psych-transcripts.txt\", \"w+\", encoding=\"utf8\") as f:\n",
    "    f.write(all_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
